{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SV5q3GcS5si8"
   },
   "outputs": [],
   "source": [
    "#Final model for personality bases chat bot, built over a seq2seq moodel in Pytorch\n",
    "#Seq2Seq code was inspired from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kXxdv9pZ5sjL"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "iP5rZbCO5zhf",
    "outputId": "46775b12-05da-4eeb-c445-a15924352d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1073750016 bytes == 0x579de000 @  0x7fb0ab68c2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
     ]
    }
   ],
   "source": [
    "#Needed to install pytorch in google colab\n",
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "Ggm3pc9u51lH",
    "outputId": "cbe376bd-a1e0-4b4d-d08d-97150623d60b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#Needed to link with google drive, if running locally with dataset file friends-final.txt , this is not needed \n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gla4Wjx95sjS"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Helper:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqLEyqwB5sjY"
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "uuo2sys25sjg",
    "outputId": "a4307b4a-ffae-4394-d1d8-92bcb1689675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60849\n",
      "[\"There's nothing to tell! He's just some guy I work with!::C'mon, you're going out with the guy! There's gotta be something wrong with him!\", \"C'mon, you're going out with the guy! There's gotta be something wrong with him!::Alright Joey, be nice. So does he have a hump? A hump and a hairpiece?\", 'Alright Joey, be nice. So does he have a hump? A hump and a hairpiece?::Wait, does he eat chalk?', \"Wait, does he eat chalk?::Just, 'cause, I don't want her to go through what I went through with Carl- oh!\", \"Just, 'cause, I don't want her to go through what I went through with Carl- oh!::Okay, everybody relax. This is not even a date. It's just two people going out to dinner and not having sex.\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Change the path to pick us friends-final.txt file provided in google drive LINK\n",
    "friends_data=pd.read_csv('/content/gdrive/My Drive/friends-final.txt',sep=\"\\t\")\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "total_count = len(friends_data.index)\n",
    "print (total_count)\n",
    "for index,row in friends_data.iterrows():\n",
    "    if (index == 0):\n",
    "        prev = row['line']\n",
    "        continue\n",
    "    if (index != total_count-1):\n",
    "        #print (friends_data.iloc[[index],[5]])\n",
    "        #x.append(friends_data.iloc[[index],[5]])\n",
    "        #y.append(friends_data.iloc[[index+1],[5]])\n",
    "        x.append(prev)\n",
    "        y.append(row['line'])\n",
    "        prev = row['line'] \n",
    "\n",
    "def getFriendPairs(X,Y):\n",
    "    pairs = []\n",
    "    for i in range(len(X)):\n",
    "        pairs.append(X[i]+\"::\"+Y[i])\n",
    "    print(pairs[:5])\n",
    "    return pairs\n",
    "\n",
    "pairs = getFriendPairs(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ar-xtvLt5sjv"
   },
   "outputs": [],
   "source": [
    "# # for pair in pairs:\n",
    "# #     print (pair)\n",
    "    \n",
    "# pairs = [print(l.split(\"::\")) for l in pairs]\n",
    "# new_pair = []\n",
    "# for pair in pairs:\n",
    "#     pair_split = pair.split(\"::\")\n",
    "#     new_pair.append(pair_split)\n",
    "# print (new_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xry7ly8C5sj1"
   },
   "outputs": [],
   "source": [
    "def readSpeakers(speak1, speak2, pairs):\n",
    "    print(\"Reading lines...\")\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split(\"::\")] for l in pairs]\n",
    "    input_speak = Helper(speak1)\n",
    "    output_speak = Helper(speak2)\n",
    "    return input_speak, output_speak, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHbNZtB55sj8"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[0].split(' ')) > 0 and \\\n",
    "        len(p[1].split(' ')) > 0 and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "wILzPUe35skC",
    "outputId": "4f0332ca-9b4f-4588-a5b8-43d574a13cb0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 60847 sentence pairs\n",
      "Trimmed to 1805 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "Speaker 1 1984\n",
      "Speaker 2 1976\n",
      "['i m with hamilton !', 'he s good with rear things bring him in too .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(speak1, speak2, pairs):\n",
    "    input_speak, output_speak, pairs = readSpeakers(speak1, speak2, pairs)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_speak.addSentence(pair[0])\n",
    "        output_speak.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_speak.name, input_speak.n_words)\n",
    "    print(output_speak.name, output_speak.n_words)\n",
    "    return input_speak, output_speak, pairs\n",
    "\n",
    "\n",
    "input_speak, output_speak, pairs = prepareData('Speaker 1', 'Speaker 2', pairs)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whYqimgO5skP"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_kCRdZO5skU"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9KTC86A5ska"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_speak, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_speak, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXpzawol5skg"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    #print(input_length)\n",
    "    target_length = target_tensor.size(0)\n",
    "    #print(target_length)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jwJS5QhN5skl"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwz7l3zZ5skq"
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcJH3iyP5sky"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "T19wW3kU5sk3",
    "outputId": "8e19f220-6fb9-4157-db65-fc41de4f35c9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joey uh omnipotent .', 'you are ? ross i m sorry . .']\n"
     ]
    }
   ],
   "source": [
    "print (pairs[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxF9qtFk5sk-"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_speak, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_speak.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKbtwlU35slG"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5EC6Bb0P5slK"
   },
   "outputs": [],
   "source": [
    "def evaluateSent(encoder, decoder, sent):\n",
    "    try :\n",
    "        output_words, attentions = evaluate(encoder, decoder, sent)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print(\">\",sent)\n",
    "        print ('<', output_sentence)\n",
    "    except:\n",
    "        print(\"Sentence contains a word which is not in our vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "MHobAX_q5slQ",
    "outputId": "72115d5f-0959-4bd6-87ed-b4202325557d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n",
      "1976\n"
     ]
    }
   ],
   "source": [
    "print (input_speak.n_words)\n",
    "print (output_speak.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3537
    },
    "colab_type": "code",
    "id": "iz1Gi4QE5slZ",
    "outputId": "c60fd3b1-3a16-49b0-8b3a-b660ea6eaee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 9s (- 32m 36s) (100 0%) 4.3768\n",
      "0m 17s (- 28m 46s) (200 1%) 3.8898\n",
      "0m 24s (- 26m 54s) (300 1%) 3.4136\n",
      "0m 32s (- 26m 11s) (400 2%) 3.4603\n",
      "0m 38s (- 25m 4s) (500 2%) 3.0488\n",
      "0m 46s (- 24m 56s) (600 3%) 3.3255\n",
      "0m 53s (- 24m 41s) (700 3%) 3.2558\n",
      "1m 0s (- 24m 17s) (800 4%) 3.0696\n",
      "1m 8s (- 24m 12s) (900 4%) 3.4840\n",
      "1m 16s (- 24m 4s) (1000 5%) 3.3250\n",
      "1m 23s (- 23m 50s) (1100 5%) 2.9206\n",
      "1m 30s (- 23m 34s) (1200 6%) 2.9876\n",
      "1m 37s (- 23m 22s) (1300 6%) 3.1870\n",
      "1m 45s (- 23m 18s) (1400 7%) 3.3092\n",
      "1m 53s (- 23m 14s) (1500 7%) 3.3011\n",
      "2m 0s (- 23m 4s) (1600 8%) 3.5207\n",
      "2m 8s (- 22m 58s) (1700 8%) 3.1314\n",
      "2m 15s (- 22m 49s) (1800 9%) 3.1835\n",
      "2m 22s (- 22m 42s) (1900 9%) 3.2997\n",
      "2m 30s (- 22m 31s) (2000 10%) 3.0749\n",
      "2m 37s (- 22m 22s) (2100 10%) 3.1566\n",
      "2m 44s (- 22m 11s) (2200 11%) 3.2103\n",
      "2m 51s (- 21m 56s) (2300 11%) 3.0570\n",
      "2m 58s (- 21m 47s) (2400 12%) 2.9792\n",
      "3m 5s (- 21m 41s) (2500 12%) 3.2593\n",
      "3m 12s (- 21m 31s) (2600 13%) 3.0443\n",
      "3m 19s (- 21m 19s) (2700 13%) 2.8801\n",
      "3m 27s (- 21m 17s) (2800 14%) 3.2883\n",
      "3m 35s (- 21m 9s) (2900 14%) 3.1199\n",
      "3m 43s (- 21m 5s) (3000 15%) 2.9876\n",
      "3m 50s (- 20m 58s) (3100 15%) 3.1610\n",
      "3m 57s (- 20m 47s) (3200 16%) 2.7851\n",
      "4m 4s (- 20m 38s) (3300 16%) 2.9099\n",
      "4m 11s (- 20m 29s) (3400 17%) 2.9250\n",
      "4m 18s (- 20m 20s) (3500 17%) 3.0384\n",
      "4m 25s (- 20m 10s) (3600 18%) 3.1523\n",
      "4m 32s (- 20m 1s) (3700 18%) 3.0328\n",
      "4m 39s (- 19m 53s) (3800 19%) 3.0897\n",
      "4m 47s (- 19m 46s) (3900 19%) 2.8163\n",
      "4m 54s (- 19m 39s) (4000 20%) 3.0485\n",
      "5m 2s (- 19m 34s) (4100 20%) 3.0608\n",
      "5m 9s (- 19m 26s) (4200 21%) 2.9815\n",
      "5m 17s (- 19m 19s) (4300 21%) 3.0431\n",
      "5m 24s (- 19m 10s) (4400 22%) 2.7153\n",
      "5m 31s (- 19m 1s) (4500 22%) 2.8853\n",
      "5m 38s (- 18m 54s) (4600 23%) 2.9640\n",
      "5m 46s (- 18m 46s) (4700 23%) 2.7858\n",
      "5m 53s (- 18m 38s) (4800 24%) 2.9080\n",
      "6m 0s (- 18m 30s) (4900 24%) 2.7694\n",
      "6m 7s (- 18m 21s) (5000 25%) 2.9931\n",
      "6m 14s (- 18m 13s) (5100 25%) 2.7121\n",
      "6m 21s (- 18m 5s) (5200 26%) 2.7528\n",
      "6m 28s (- 17m 56s) (5300 26%) 2.6405\n",
      "6m 34s (- 17m 47s) (5400 27%) 2.7821\n",
      "6m 42s (- 17m 39s) (5500 27%) 3.1178\n",
      "6m 48s (- 17m 31s) (5600 28%) 2.7438\n",
      "6m 56s (- 17m 24s) (5700 28%) 2.9332\n",
      "7m 3s (- 17m 17s) (5800 28%) 3.0930\n",
      "7m 10s (- 17m 8s) (5900 29%) 2.6224\n",
      "7m 17s (- 17m 0s) (6000 30%) 2.8677\n",
      "7m 24s (- 16m 52s) (6100 30%) 2.7114\n",
      "7m 31s (- 16m 45s) (6200 31%) 2.9420\n",
      "7m 38s (- 16m 36s) (6300 31%) 2.5151\n",
      "7m 44s (- 16m 27s) (6400 32%) 2.4052\n",
      "7m 52s (- 16m 20s) (6500 32%) 2.7193\n",
      "7m 58s (- 16m 12s) (6600 33%) 2.6997\n",
      "8m 5s (- 16m 3s) (6700 33%) 2.4426\n",
      "8m 12s (- 15m 56s) (6800 34%) 2.7709\n",
      "8m 19s (- 15m 48s) (6900 34%) 2.6703\n",
      "8m 27s (- 15m 42s) (7000 35%) 3.0500\n",
      "8m 34s (- 15m 35s) (7100 35%) 2.7097\n",
      "8m 42s (- 15m 29s) (7200 36%) 2.6347\n",
      "8m 49s (- 15m 22s) (7300 36%) 2.7526\n",
      "8m 57s (- 15m 14s) (7400 37%) 2.6550\n",
      "9m 4s (- 15m 7s) (7500 37%) 2.7060\n",
      "9m 11s (- 15m 0s) (7600 38%) 2.6670\n",
      "9m 19s (- 14m 54s) (7700 38%) 2.9411\n",
      "9m 27s (- 14m 46s) (7800 39%) 2.7320\n",
      "9m 33s (- 14m 37s) (7900 39%) 2.4150\n",
      "9m 40s (- 14m 30s) (8000 40%) 2.4567\n",
      "9m 47s (- 14m 23s) (8100 40%) 2.4052\n",
      "9m 55s (- 14m 16s) (8200 41%) 2.4068\n",
      "10m 3s (- 14m 10s) (8300 41%) 2.7193\n",
      "10m 10s (- 14m 2s) (8400 42%) 2.5644\n",
      "10m 17s (- 13m 55s) (8500 42%) 2.5916\n",
      "10m 24s (- 13m 48s) (8600 43%) 2.7052\n",
      "10m 32s (- 13m 41s) (8700 43%) 2.5506\n",
      "10m 39s (- 13m 33s) (8800 44%) 2.6857\n",
      "10m 47s (- 13m 27s) (8900 44%) 2.6805\n",
      "10m 54s (- 13m 20s) (9000 45%) 2.7550\n",
      "11m 1s (- 13m 12s) (9100 45%) 2.5945\n",
      "11m 8s (- 13m 4s) (9200 46%) 2.4478\n",
      "11m 16s (- 12m 57s) (9300 46%) 2.4473\n",
      "11m 23s (- 12m 50s) (9400 47%) 2.4493\n",
      "11m 30s (- 12m 43s) (9500 47%) 2.1681\n",
      "11m 36s (- 12m 35s) (9600 48%) 2.3504\n",
      "11m 44s (- 12m 28s) (9700 48%) 2.6118\n",
      "11m 52s (- 12m 21s) (9800 49%) 2.4483\n",
      "11m 59s (- 12m 13s) (9900 49%) 2.7699\n",
      "12m 6s (- 12m 6s) (10000 50%) 2.4372\n",
      "12m 14s (- 11m 59s) (10100 50%) 2.7093\n",
      "12m 22s (- 11m 53s) (10200 51%) 2.4236\n",
      "12m 28s (- 11m 45s) (10300 51%) 2.2248\n",
      "12m 36s (- 11m 37s) (10400 52%) 2.5329\n",
      "12m 43s (- 11m 30s) (10500 52%) 2.3900\n",
      "12m 51s (- 11m 23s) (10600 53%) 2.6825\n",
      "12m 58s (- 11m 16s) (10700 53%) 2.3199\n",
      "13m 6s (- 11m 10s) (10800 54%) 2.6396\n",
      "13m 15s (- 11m 4s) (10900 54%) 2.6247\n",
      "13m 23s (- 10m 57s) (11000 55%) 2.5263\n",
      "13m 31s (- 10m 50s) (11100 55%) 2.3804\n",
      "13m 39s (- 10m 43s) (11200 56%) 2.4812\n",
      "13m 46s (- 10m 36s) (11300 56%) 2.3323\n",
      "13m 54s (- 10m 29s) (11400 56%) 2.5142\n",
      "14m 2s (- 10m 22s) (11500 57%) 2.1918\n",
      "14m 9s (- 10m 15s) (11600 57%) 2.2059\n",
      "14m 17s (- 10m 8s) (11700 58%) 2.3722\n",
      "14m 25s (- 10m 1s) (11800 59%) 2.4372\n",
      "14m 32s (- 9m 54s) (11900 59%) 2.0237\n",
      "14m 41s (- 9m 47s) (12000 60%) 2.3308\n",
      "14m 48s (- 9m 40s) (12100 60%) 2.4201\n",
      "14m 56s (- 9m 33s) (12200 61%) 2.4887\n",
      "15m 3s (- 9m 25s) (12300 61%) 2.2083\n",
      "15m 12s (- 9m 19s) (12400 62%) 2.2929\n",
      "15m 19s (- 9m 11s) (12500 62%) 1.8424\n",
      "15m 27s (- 9m 4s) (12600 63%) 2.3150\n",
      "15m 34s (- 8m 57s) (12700 63%) 2.1223\n",
      "15m 41s (- 8m 49s) (12800 64%) 2.0337\n",
      "15m 49s (- 8m 42s) (12900 64%) 2.0950\n",
      "15m 56s (- 8m 35s) (13000 65%) 2.1343\n",
      "16m 3s (- 8m 27s) (13100 65%) 2.1090\n",
      "16m 11s (- 8m 20s) (13200 66%) 2.0829\n",
      "16m 19s (- 8m 13s) (13300 66%) 2.3082\n",
      "16m 27s (- 8m 6s) (13400 67%) 2.1425\n",
      "16m 34s (- 7m 58s) (13500 67%) 2.0699\n",
      "16m 42s (- 7m 51s) (13600 68%) 2.2789\n",
      "16m 50s (- 7m 44s) (13700 68%) 2.1797\n",
      "16m 57s (- 7m 37s) (13800 69%) 1.9576\n",
      "17m 5s (- 7m 29s) (13900 69%) 2.0961\n",
      "17m 12s (- 7m 22s) (14000 70%) 2.0882\n",
      "17m 20s (- 7m 15s) (14100 70%) 2.0896\n",
      "17m 27s (- 7m 8s) (14200 71%) 2.0050\n",
      "17m 36s (- 7m 1s) (14300 71%) 2.0645\n",
      "17m 44s (- 6m 54s) (14400 72%) 1.9101\n",
      "17m 53s (- 6m 47s) (14500 72%) 2.1361\n",
      "18m 0s (- 6m 39s) (14600 73%) 1.9311\n",
      "18m 8s (- 6m 32s) (14700 73%) 2.0825\n",
      "18m 15s (- 6m 25s) (14800 74%) 2.1473\n",
      "18m 24s (- 6m 17s) (14900 74%) 1.9941\n",
      "18m 31s (- 6m 10s) (15000 75%) 1.9943\n",
      "18m 39s (- 6m 3s) (15100 75%) 2.0156\n",
      "18m 47s (- 5m 56s) (15200 76%) 1.6847\n",
      "18m 55s (- 5m 48s) (15300 76%) 2.0577\n",
      "19m 3s (- 5m 41s) (15400 77%) 1.6076\n",
      "19m 11s (- 5m 34s) (15500 77%) 2.0629\n",
      "19m 18s (- 5m 26s) (15600 78%) 1.7394\n",
      "19m 26s (- 5m 19s) (15700 78%) 1.8262\n",
      "19m 33s (- 5m 12s) (15800 79%) 1.7400\n",
      "19m 41s (- 5m 4s) (15900 79%) 1.8418\n",
      "19m 49s (- 4m 57s) (16000 80%) 1.6903\n",
      "19m 57s (- 4m 49s) (16100 80%) 1.7954\n",
      "20m 5s (- 4m 42s) (16200 81%) 1.8558\n",
      "20m 13s (- 4m 35s) (16300 81%) 1.7033\n",
      "20m 21s (- 4m 28s) (16400 82%) 1.8193\n",
      "20m 29s (- 4m 20s) (16500 82%) 1.8113\n",
      "20m 37s (- 4m 13s) (16600 83%) 1.9575\n",
      "20m 45s (- 4m 6s) (16700 83%) 1.7791\n",
      "20m 53s (- 3m 58s) (16800 84%) 1.7683\n",
      "21m 0s (- 3m 51s) (16900 84%) 1.6389\n",
      "21m 8s (- 3m 43s) (17000 85%) 2.2569\n",
      "21m 16s (- 3m 36s) (17100 85%) 1.4267\n",
      "21m 24s (- 3m 29s) (17200 86%) 1.6277\n",
      "21m 32s (- 3m 21s) (17300 86%) 1.3796\n",
      "21m 40s (- 3m 14s) (17400 87%) 1.5429\n",
      "21m 48s (- 3m 6s) (17500 87%) 1.6618\n",
      "21m 56s (- 2m 59s) (17600 88%) 1.5156\n",
      "22m 3s (- 2m 52s) (17700 88%) 1.5790\n",
      "22m 11s (- 2m 44s) (17800 89%) 2.0320\n",
      "22m 19s (- 2m 37s) (17900 89%) 1.7589\n",
      "22m 27s (- 2m 29s) (18000 90%) 1.5511\n",
      "22m 35s (- 2m 22s) (18100 90%) 1.5710\n",
      "22m 42s (- 2m 14s) (18200 91%) 1.3696\n",
      "22m 51s (- 2m 7s) (18300 91%) 1.5269\n",
      "22m 59s (- 1m 59s) (18400 92%) 1.6856\n",
      "23m 7s (- 1m 52s) (18500 92%) 1.6849\n",
      "23m 15s (- 1m 45s) (18600 93%) 1.4207\n",
      "23m 23s (- 1m 37s) (18700 93%) 1.6477\n",
      "23m 31s (- 1m 30s) (18800 94%) 1.4107\n",
      "23m 39s (- 1m 22s) (18900 94%) 1.5904\n",
      "23m 47s (- 1m 15s) (19000 95%) 1.6580\n",
      "23m 56s (- 1m 7s) (19100 95%) 1.6338\n",
      "24m 5s (- 1m 0s) (19200 96%) 1.6283\n",
      "24m 13s (- 0m 52s) (19300 96%) 1.5714\n",
      "24m 21s (- 0m 45s) (19400 97%) 1.6385\n",
      "24m 29s (- 0m 37s) (19500 97%) 1.6459\n",
      "24m 36s (- 0m 30s) (19600 98%) 1.4689\n",
      "24m 44s (- 0m 22s) (19700 98%) 1.4384\n",
      "24m 52s (- 0m 15s) (19800 99%) 1.3461\n",
      "25m 0s (- 0m 7s) (19900 99%) 1.4754\n",
      "25m 8s (- 0m 0s) (20000 100%) 1.4292\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 512\n",
    "encoder1 = EncoderRNN(input_speak.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_speak.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 20000, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCqsVyB75slg"
   },
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), \"encoder1short20000\")\n",
    "torch.save(attn_decoder1.state_dict(), \"attn_decoder1short20000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "-yQu5dY35slt",
    "outputId": "fb50ce57-dc57-4719-be22-c7a85d2068a2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> yeah .\n",
      "< i m gonna use it up . <EOS>\n",
      "> how you doing\n",
      "< i m okay go out okay ? <EOS>\n",
      "> where is everyone .\n",
      "< he s in a oh ! <EOS>\n",
      "> who are you ?\n",
      "< i m not finished my you here ross phoebe s friend from the ! i m just gonna be just fine ! <EOS>\n",
      "> when is the next one coming .\n",
      "< i m gonna marry you you re the best . <EOS>\n",
      "> do you know ?\n",
      "< i m sorry about <EOS>\n",
      "> nice .\n",
      "< i m so sorry . <EOS>\n",
      "> how are you .\n",
      "< i m fine you re i m m m i m m with you ? <EOS>\n"
     ]
    }
   ],
   "source": [
    "evaluateSent(encoder1,attn_decoder1, \"yeah .\")\n",
    "evaluateSent(encoder1,attn_decoder1, \"how you doing\")\n",
    "evaluateSent(encoder1,attn_decoder1, \"where is everyone .\")\n",
    "evaluateSent(encoder1,attn_decoder1, \"who are you ?\")\n",
    "evaluateSent(encoder1,attn_decoder1, \"when is the next one coming .\")\n",
    "evaluateSent(encoder1,attn_decoder1, \"do you know ?\")\n",
    "evaluateSent(encoder1,attn_decoder1, \"nice .\")\n",
    "evaluateSent(encoder1,attn_decoder1, \"how are you .\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FC3nmKWm5sl0",
    "outputId": "dc24f272-32f8-40be-bfc1-ea46b9c6abd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence contains a word which is not in our vocabulary\n"
     ]
    }
   ],
   "source": [
    "# encoder = torch.load(\"encoder1\")\n",
    "# #encoder.eval()\n",
    "\n",
    "# #decoder = TheModelClass(*args, **kwargs)\n",
    "# decoder =torch.load(\"attn_decoder1\")\n",
    "# evaluateSent(encoder,decoder, \"yeah .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "colab_type": "code",
    "id": "-Y4-zrwf5smB",
    "outputId": "47fe6ee4-a8bc-4b28-b647-48891dda1db3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> maybe a hello kitty doll the ability to walk . . .\n",
      "= i m gonna get back to retraining .\n",
      "< i m gonna get back to retraining . <EOS>\n",
      "\n",
      "> ok . smelly cat smell ly cat what are they feeding you ? smelly cat oh woah oh my god . i mean like who was that ?\n",
      "= they re your backup singers . . . beind you .\n",
      "< they re your backup singers . . . . . <EOS>\n",
      "\n",
      "> no i don t think we ll be doing that .\n",
      "= we re not gonna do that .\n",
      "< we re gonna do that . <EOS>\n",
      "\n",
      "> spoon ? so what do you think ?\n",
      "= i m torn between my integrity and my desire to avoid a beating . but i must be honest your soap is abysmal .\n",
      "< i m on in a <EOS>\n",
      "\n",
      "> yeah i just ordered a beer !\n",
      "= you re straight . i get it .\n",
      "< you re a . . . . <EOS>\n",
      "\n",
      "> told ya .\n",
      "= they are needy they are jumpy and you can t tell what they are thinking and that scares me a little bit .\n",
      "< i m going to the bathroom . <EOS>\n",
      "\n",
      "> well you might want to tell him it sounds like his wife is gay .\n",
      "= she is not . . . she s gay . oh my god . she is so gay ! i can t believe this .\n",
      "< she is not . . . . . . . . . . . . . <EOS>\n",
      "\n",
      "> it ll be okay joe .\n",
      "= i m sorry i just . . . i like things the way they are .\n",
      "< i m sorry . i i just . . . i m just . . . <EOS>\n",
      "\n",
      "> are you lying ? is this like that time you tried to convince us that you were a doctor ?\n",
      "= i am a doctor ! y know what ? i m just gonna go and talk to rachel myself .\n",
      "< i m a doctor ! y know what ? just go s just gonna go just and talk to rachel . <EOS>\n",
      "\n",
      "> you look so beautiful .\n",
      "= i m so happy for me .\n",
      "< i m not ! i m just . the ! <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FriendsChatBot_over_seq2seq.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
